{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import v2\n",
    "from torchmetrics import functional as mF\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the baseline\n",
    "\n",
    "Simple MLP with 64 hidden dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(MNIST):\n",
    "    pass\n",
    "\n",
    "class DigitDataModule(L.LightningDataModule):\n",
    "    # TAKEN FROM https://github.com/sxg/PyTorch-Lightning-MNIST-Classifier/blob/main/DigitDataModule.py\n",
    "    def __init__(self, dataset, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.transform = v2.Compose([\n",
    "          v2.ToTensor(),\n",
    "          v2.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.dataset(\"MNIST\", train=True, download=True)\n",
    "        self.dataset(\"MNIST\", train=False, download=True)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            full_set = self.dataset(root=\"MNIST\", train=True, transform=self.transform)\n",
    "            train_set_size = int(len(full_set) * 0.8)\n",
    "            val_set_size = len(full_set) - train_set_size\n",
    "            seed = torch.Generator().manual_seed(42)\n",
    "            self.train_set, self.val_set = data.random_split(full_set, [train_set_size, val_set_size], generator=seed)\n",
    "        elif stage == \"test\":\n",
    "            self.test_set = self.dataset(root=\"MNIST\", train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(self.train_set, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(self.val_set, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data.DataLoader(self.test_set, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "class DigitModuleBaseline(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.classifier = nn.Linear(64, 10)\n",
    "\n",
    "    def data_step(self, batch, log_name):\n",
    "        x, y = batch\n",
    "        \n",
    "        latent = self.mlp(x.view(-1, 784))\n",
    "        pred = self.classifier(latent)\n",
    "\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(f\"{log_name}/loss\", loss, prog_bar=True)\n",
    "\n",
    "        acc = mF.accuracy(pred, y, task=\"multiclass\", num_classes=10)\n",
    "        self.log(f\"{log_name}/accuracy\", acc, prog_bar=True)\n",
    "\n",
    "        return pred, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pred, loss, _ = self.data_step(batch, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pred, loss, _ = self.data_step(batch, \"valid\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pred, loss, acc = self.data_step(batch, \"test\")\n",
    "        return {\"loss\": loss, \"acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "dm = DigitDataModule(MNISTDataset, batch_size=64)\n",
    "\n",
    "model = DigitModuleBaseline()\n",
    "trainer = L.Trainer(\n",
    "  #devices=[0], \n",
    "  accelerator=\"cpu\",\n",
    "  max_epochs=5)\n",
    "\n",
    "#trainer.fit(model, datamodule=dm)\n",
    "#trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's sandra it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import rdflib\n",
    "from sandra.ontology import KnowledgeBase\n",
    "from sandra.torch import ReasonerModule\n",
    "\n",
    "dc = KnowledgeBase.from_graph(rdflib.Graph().parse(\"digital_display.ttl\"))\n",
    "reasoner = ReasonerModule(dc, t=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the new model with the projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SandraDigitModule(DigitModuleBaseline):\n",
    "    def __init__(self, reasoner):\n",
    "        super().__init__()\n",
    "        self.reasoner = reasoner\n",
    "        sandra_dim = len(self.reasoner._encoded_attrs)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(64, sandra_dim), \n",
    "            nn.Tanh())\n",
    "\n",
    "        self.classifier = nn.Linear(64 + len(self.reasoner.kb.frames), 10)\n",
    "        \n",
    "    def data_step(self, batch, log_name):\n",
    "        x, y = batch\n",
    "\n",
    "        x = x.view(-1, 784)\n",
    "        z = self.mlp(x)\n",
    "        proj = self.proj(z)\n",
    "        \n",
    "        inf = self.reasoner(proj)\n",
    "        y_inf = self.reasoner(self.reasoner._frame_gate[y])\n",
    "        inference_loss = F.mse_loss(inf, y_inf)\n",
    "        self.log(f\"{log_name}/inference_loss\", inference_loss, prog_bar=True)\n",
    "\n",
    "        pred = self.classifier(torch.cat([z, inf], dim=-1))\n",
    "        classification_loss = F.cross_entropy(pred, y)\n",
    "        self.log(f\"{log_name}/classification_loss\", classification_loss, prog_bar=True)\n",
    "\n",
    "        acc = mF.accuracy(pred, y, task=\"multiclass\", num_classes=10)\n",
    "        self.log(f\"{log_name}/accuracy\", acc, prog_bar=True)\n",
    "        \n",
    "        loss = classification_loss + inference_loss\n",
    "        \n",
    "        return pred, loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type           | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | mlp        | Sequential     | 50.2 K | train\n",
      "1 | classifier | Linear         | 720    | train\n",
      "2 | reasoner   | ReasonerModule | 0      | eval \n",
      "3 | proj       | Linear         | 455    | train\n",
      "------------------------------------------------------\n",
      "51.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.4 K    Total params\n",
      "0.206     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n28div/miniconda3/envs/sandra/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:06<00:00, 120.79it/s, v_num=225, train/inference_loss=0.0306, train/classification_loss=0.139, train/accuracy=0.953, valid/inference_loss=0.0308, valid/classification_loss=0.169, valid/accuracy=0.951] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:06<00:00, 120.68it/s, v_num=225, train/inference_loss=0.0306, train/classification_loss=0.139, train/accuracy=0.953, valid/inference_loss=0.0308, valid/classification_loss=0.169, valid/accuracy=0.951]\n",
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 264.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9562000036239624     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test/classification_loss  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14902405440807343    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/inference_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.028598645702004433    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9562000036239624    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest/classification_loss \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14902405440807343   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/inference_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.028598645702004433   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/inference_loss': 0.028598645702004433,\n",
       "  'test/classification_loss': 0.14902405440807343,\n",
       "  'test/accuracy': 0.9562000036239624}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SandraDigitModule(reasoner)\n",
    "trainer = L.Trainer(\n",
    "  #devices=[0], \n",
    "  #accelerator=\"gpu\",\n",
    "  max_epochs=5)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor(1)\n",
      "Inferred situation: \n",
      "\t Attribute(name='https://w3id.org/geometryofmeaning/sandra/digital-display#SegmentAOn')\n",
      "\t Attribute(name='https://w3id.org/geometryofmeaning/sandra/digital-display#SegmentBOn')\n",
      "\t Attribute(name='https://w3id.org/geometryofmeaning/sandra/digital-display#SegmentCOn')\n",
      "\t Attribute(name='https://w3id.org/geometryofmeaning/sandra/digital-display#SegmentDOn')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAGdCAYAAAD0YQ2BAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOY0lEQVR4nO3dUYhc9b3A8d9u0kxsO7sYJdElExSKiOYm0piE4EXSujXklmDuUx+EhjwIwq407Ivui6EPZX0SSw2ptLa+NBgqrILcGiRtsggG44ZALNSLVOjKNom+zG72YRIzcx96+7t3q8Y9686cWffzgfNwDv/J+THZfDlzdibT02q1WgEQEb1lDwB0D0EAkiAASRCAJAhAEgQgCQKQBAFIqzt9wmazGdPT01GtVqOnp6fTp4cVp9VqxezsbAwMDERv742vAToehOnp6ajVap0+Lax4U1NTsXHjxhuu6XgQqtVqRET8e/xHrI5vdPr0y8r4f18oe4Rl4T/v+reyR+hqn8a1eCv+K//t3UjHg/DPlwmr4xuxukcQbqSv6hbPQvg5+hL/+2mlhbxE9xMHJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAWlQQjhw5EnfccUesXbs2du7cGe+8885SzwWUoHAQjh8/HiMjI3H48OE4d+5cbN26Nfbs2ROXL19ux3xABxUOwrPPPhuPPfZYHDx4MO6555745S9/Gd/85jfjN7/5TTvmAzqoUBCuXr0ak5OTMTg4+H9/QG9vDA4Oxttvv73kwwGdtbrI4k8++SSuX78eGzZsmHd8w4YN8Ze//OVzH9NoNKLRaOT+zMzMIsYEOqHtv2UYGxuL/v7+3Gq1WrtPCSxSoSDceuutsWrVqrh06dK845cuXYrbbrvtcx8zOjoa9Xo9t6mpqcVPC7RVoSCsWbMmtm3bFidPnsxjzWYzTp48Gbt27frcx1Qqlejr65u3Ad2p0D2EiIiRkZE4cOBA3H///bFjx4547rnnYm5uLg4ePNiO+YAOKhyEH/3oR/Hxxx/H008/HRcvXoz77rsv3njjjc/caASWn8JBiIgYHh6O4eHhpZ4FKJnPMgBJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiL+jp4OmPPwH1lj7AsnJg+X/YIXW1mthk337Wwta4QgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAAqXAQJiYmYt++fTEwMBA9PT3x6quvtmEsoAyFgzA3Nxdbt26NI0eOtGMeoESriz5g7969sXfv3nbMApTMPQQgFb5CKKrRaESj0cj9mZmZdp8SWKS2XyGMjY1Ff39/brVard2nBBap7UEYHR2Ner2e29TUVLtPCSxS218yVCqVqFQq7T4NsAQKB+HKlSvxwQcf5P6HH34Y58+fj3Xr1sWmTZuWdDigswoH4d13343vfe97uT8yMhIREQcOHIiXXnppyQYDOq9wEHbv3h2tVqsdswAl8z4EIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAAqfDXwUO32TNwX9kjdLVPW9ci4q8LWusKAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAoFYWxsLLZv3x7VajXWr18f+/fvj/fff79dswEdVigIp0+fjqGhoThz5ky8+eabce3atXj44Ydjbm6uXfMBHbS6yOI33nhj3v5LL70U69evj8nJyXjwwQeXdDCg8woF4V/V6/WIiFi3bt0Xrmk0GtFoNHJ/Zmbmq5wSaKNF31RsNptx6NCheOCBB2Lz5s1fuG5sbCz6+/tzq9Vqiz0l0GaLDsLQ0FC899578fLLL99w3ejoaNTr9dympqYWe0qgzRb1kmF4eDhef/31mJiYiI0bN95wbaVSiUqlsqjhgM4qFIRWqxVPPPFEjI+Px6lTp+LOO+9s11xACQoFYWhoKI4dOxavvfZaVKvVuHjxYkRE9Pf3x0033dSWAYHOKXQP4ejRo1Gv12P37t1x++2353b8+PF2zQd0UOGXDMDXl88yAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIW+Dh660Ynp82WP0NVmZptx810LW+sKAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAoF4ejRo7Fly5bo6+uLvr6+2LVrV/zhD39o12xAhxUKwsaNG+OZZ56JycnJePfdd+P73/9+PPLII/HnP/+5XfMBHbS6yOJ9+/bN2//Zz34WR48ejTNnzsS99967pIMBnVcoCP/f9evX4/e//33Mzc3Frl27vnBdo9GIRqOR+zMzM4s9JdBmhW8qXrhwIb797W9HpVKJxx9/PMbHx+Oee+75wvVjY2PR39+fW61W+0oDA+3T02q1WkUecPXq1fjb3/4W9Xo9Xnnllfj1r38dp0+f/sIofN4VQq1Wi93xSKzu+cZXmx4i4sT0+bJH6Gozs824+a6/Rr1ej76+vhuuLfySYc2aNfGd73wnIiK2bdsWZ8+ejZ///OfxwgsvfO76SqUSlUql6GmAEnzl9yE0m815VwDA8lXoCmF0dDT27t0bmzZtitnZ2Th27FicOnUqTpw40a75gA4qFITLly/Hj3/84/j73/8e/f39sWXLljhx4kT84Ac/aNd8QAcVCsKLL77YrjmALuCzDEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAFKhr4Ons05Mny97hGVhz8B9ZY/Q1T5tXYuIvy5orSsEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhA+kpBeOaZZ6KnpycOHTq0ROMAZVp0EM6ePRsvvPBCbNmyZSnnAUq0qCBcuXIlHn300fjVr34VN99881LPBJRkUUEYGhqKH/7whzE4OPilaxuNRszMzMzbgO60uugDXn755Th37lycPXt2QevHxsbipz/9aeHBgM4rdIUwNTUVP/nJT+J3v/tdrF27dkGPGR0djXq9ntvU1NSiBgXar9AVwuTkZFy+fDm++93v5rHr16/HxMREPP/889FoNGLVqlXzHlOpVKJSqSzNtEBbFQrCQw89FBcuXJh37ODBg3H33XfHk08++ZkYAMtLoSBUq9XYvHnzvGPf+ta34pZbbvnMcWD58U5FIBX+LcO/OnXq1BKMAXQDVwhAEgQgCQKQBAFIggAkQQCSIABJEIAkCEASBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSIIAJEEAkiAASRCAJAhAEgQgCQKQBAFIggCkr/ztz0W1Wq2IiPg0rkW0On325WVmtln2CMvCp61rZY/Q1T6Nfzw///y3dyM9rYWsWkIfffRR1Gq1Tp4SiIipqanYuHHjDdd0PAjNZjOmp6ejWq1GT09PJ0/9hWZmZqJWq8XU1FT09fWVPU5X8hwtTDc+T61WK2ZnZ2NgYCB6e298l6DjLxl6e3u/tFJl6evr65q/xG7lOVqYbnue+vv7F7TOTUUgCQKQBCEiKpVKHD58OCqVStmjdC3P0cIs9+ep4zcVge7lCgFIggAkQQCSIABpxQfhyJEjcccdd8TatWtj586d8c4775Q9UteZmJiIffv2xcDAQPT09MSrr75a9khdZ2xsLLZv3x7VajXWr18f+/fvj/fff7/ssQpb0UE4fvx4jIyMxOHDh+PcuXOxdevW2LNnT1y+fLns0brK3NxcbN26NY4cOVL2KF3r9OnTMTQ0FGfOnIk333wzrl27Fg8//HDMzc2VPVohK/rXjjt37ozt27fH888/HxH/+JxFrVaLJ554Ip566qmSp+tOPT09MT4+Hvv37y97lK728ccfx/r16+P06dPx4IMPlj3Ogq3YK4SrV6/G5ORkDA4O5rHe3t4YHByMt99+u8TJ+Dqo1+sREbFu3bqSJylmxQbhk08+ievXr8eGDRvmHd+wYUNcvHixpKn4Omg2m3Ho0KF44IEHYvPmzWWPU0jHP+0IX3dDQ0Px3nvvxVtvvVX2KIWt2CDceuutsWrVqrh06dK845cuXYrbbrutpKlY7oaHh+P111+PiYmJrv2Y/42s2JcMa9asiW3btsXJkyfzWLPZjJMnT8auXbtKnIzlqNVqxfDwcIyPj8cf//jHuPPOO8seaVFW7BVCRMTIyEgcOHAg7r///tixY0c899xzMTc3FwcPHix7tK5y5cqV+OCDD3L/ww8/jPPnz8e6deti06ZNJU7WPYaGhuLYsWPx2muvRbVazftQ/f39cdNNN5U8XQGtFe4Xv/hFa9OmTa01a9a0duzY0Tpz5kzZI3WdP/3pT634x3+JO287cOBA2aN1jc97fiKi9dvf/rbs0QpZ0e9DAOZbsfcQgM8SBCAJApAEAUiCACRBAJIgAEkQgCQIQBIEIAkCkAQBSP8DPmTdGo2po7YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IDX = 6\n",
    "image, label = next(iter(dm.train_dataloader()))\n",
    "\n",
    "z = model.mlp(image[IDX].reshape(1, -1))\n",
    "proj = model.proj(z)\n",
    "\n",
    "inf = reasoner(proj)\n",
    "#pred = model.classifier(torch.cat([latent, inf], dim=-1))\n",
    "#pred = model.classifier(latent)\n",
    "\n",
    "#print(\"Pred\", pred.argmax())\n",
    "print(\"Target\", label[IDX])\n",
    "\n",
    "inferred_situation = [dc.attributes[i] for i in torch.where((proj @ reasoner._encoded_attrs) > 0)[1].tolist()]\n",
    "print(\"Inferred situation: \\n\\t\", \"\\n\\t \".join([str(e) for e in inferred_situation]))\n",
    "\n",
    "seven_seg = [1 if r in inferred_situation else 0 for r in dc.attributes]\n",
    "seven_seg = [0, seven_seg[0], 0, \n",
    "             seven_seg[5], 0, seven_seg[1],\n",
    "             0, seven_seg[-1], 0,\n",
    "             seven_seg[4], 0, seven_seg[2],\n",
    "             0, seven_seg[3], 0 ]\n",
    "plt.imshow(torch.tensor(seven_seg).reshape(5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatedDigitDataModule(DigitDataModule):\n",
    "    # TAKEN FROM https://github.com/sxg/PyTorch-Lightning-MNIST-Classifier/blob/main/DigitDataModule.py\n",
    "    def __init__(self, dataset, train_rotations, test_rotations, batch_size=64):\n",
    "        super().__init__(dataset, batch_size=batch_size)\n",
    "        base_transforms = [\n",
    "          v2.ToTensor(),\n",
    "          v2.Normalize((0.5,), (0.5,))\n",
    "        ]\n",
    "\n",
    "        self.train_transform = v2.Compose(\n",
    "            base_transforms + [ v2.Lambda(lambda i: v2.functional.rotate(i, choice(train_rotations))), ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = v2.Compose(\n",
    "            base_transforms + [ v2.Lambda(lambda i: v2.functional.rotate(i, choice(test_rotations))), ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            full_set = self.dataset(root=\"MNIST\", train=True, transform=self.train_transform)\n",
    "            train_set_size = int(len(full_set) * 0.8)\n",
    "            val_set_size = len(full_set) - train_set_size\n",
    "            seed = torch.Generator().manual_seed(42)\n",
    "            self.train_set, self.val_set = data.random_split(full_set, [train_set_size, val_set_size], generator=seed)\n",
    "        elif stage == \"test\":\n",
    "            self.test_set = self.dataset(root=\"MNIST\", train=False, transform=self.test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n28div/miniconda3/envs/sandra/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type       | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | mlp        | Sequential | 50.2 K | train\n",
      "1 | classifier | Linear     | 650    | train\n",
      "--------------------------------------------------\n",
      "50.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "50.9 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:05<00:00, 127.27it/s, v_num=232, train/loss=0.131, train/accuracy=0.969, valid/loss=0.201, valid/accuracy=0.941] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:05<00:00, 127.16it/s, v_num=232, train/loss=0.131, train/accuracy=0.969, valid/loss=0.201, valid/accuracy=0.941]\n",
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 283.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5424000024795532     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7742990255355835     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5424000024795532    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7742990255355835    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/loss': 1.7742990255355835, 'test/accuracy': 0.5424000024795532}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dm = RotatedDigitDataModule(MNISTDataset, (30, 45), (0, 90), batch_size=64)\n",
    "dm = RotatedDigitDataModule(MNISTDataset, (30, 45, 60), (0, 90), batch_size=64)\n",
    "\n",
    "model = DigitModuleBaseline()\n",
    "trainer = L.Trainer(\n",
    "  #devices=[0], \n",
    "  accelerator=\"cpu\",\n",
    "  max_epochs=5)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type           | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | mlp        | Sequential     | 50.2 K | train\n",
      "1 | classifier | Linear         | 750    | train\n",
      "2 | reasoner   | ReasonerModule | 0      | eval \n",
      "3 | proj       | Sequential     | 455    | train\n",
      "------------------------------------------------------\n",
      "51.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.4 K    Total params\n",
      "0.206     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:07<00:00, 105.52it/s, v_num=233, train/inference_loss=0.0209, train/classification_loss=0.151, train/accuracy=0.953, valid/inference_loss=0.0258, valid/classification_loss=0.191, valid/accuracy=0.944] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:07<00:00, 105.42it/s, v_num=233, train/inference_loss=0.0209, train/classification_loss=0.151, train/accuracy=0.953, valid/inference_loss=0.0258, valid/classification_loss=0.191, valid/accuracy=0.944]\n",
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 248.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5396000146865845     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test/classification_loss  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.810421347618103     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test/inference_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.06789376586675644    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5396000146865845    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest/classification_loss \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.810421347618103    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test/inference_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06789376586675644   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/inference_loss': 0.06789376586675644,\n",
       "  'test/classification_loss': 1.810421347618103,\n",
       "  'test/accuracy': 0.5396000146865845}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SandraDigitModule(reasoner)\n",
    "trainer = L.Trainer(\n",
    "  #devices=[0], \n",
    "  #accelerator=\"gpu\",\n",
    "  max_epochs=5)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
